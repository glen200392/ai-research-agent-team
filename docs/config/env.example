# =============================================================================
# AI Research Federation — Environment Configuration Template
# =============================================================================
# Copy this file to .env and fill in your values before deploying.
# NEVER commit .env to version control.
# Last updated: 2026-02-21 | Version: 2.0.0
# =============================================================================

# -----------------------------------------------------------------------------
# [REQUIRED] LLM & AI Services
# -----------------------------------------------------------------------------
OPENAI_API_KEY=sk-...                        # GPT-4o / GPT-4-turbo
ANTHROPIC_API_KEY=sk-ant-...                 # Claude 3.5 Sonnet (fallback)
GOOGLE_API_KEY=...                           # Gemini 1.5 Pro (fallback)

# Primary LLM model selection (default: gpt-4o)
LLM_PRIMARY_MODEL=gpt-4o
LLM_FALLBACK_MODEL=claude-3-5-sonnet-20241022
LLM_TEMPERATURE=0.3                          # Lower = more deterministic outputs

# -----------------------------------------------------------------------------
# [REQUIRED] Search & Web Services
# -----------------------------------------------------------------------------
SEARCH_API_KEY=...                           # Tavily / SerpAPI / Bing Search
SEARCH_PROVIDER=tavily                       # tavily | serpapi | bing

# -----------------------------------------------------------------------------
# [REQUIRED] Notification
# -----------------------------------------------------------------------------
NOTIFY_EMAIL=glen200392@gmail.com            # Primary notification recipient
NOTIFY_EMAIL_CC=                             # Optional CC (comma-separated)
SMTP_HOST=smtp.gmail.com                     # SMTP server (if not using Nebula)
SMTP_PORT=587
SMTP_USER=
SMTP_PASS=

# Email provider selection
EMAIL_PROVIDER=nebula                        # nebula | smtp | sendgrid | mailgun
SENDGRID_API_KEY=                            # Only if EMAIL_PROVIDER=sendgrid
MAILGUN_API_KEY=                             # Only if EMAIL_PROVIDER=mailgun

# -----------------------------------------------------------------------------
# [REQUIRED] Storage & Output
# -----------------------------------------------------------------------------
OUTPUT_BACKEND=local                         # local | github | s3 | gcs
OUTPUT_BASE_PATH=docs                        # Base path for all output files

# GitHub (if OUTPUT_BACKEND=github)
GITHUB_TOKEN=ghp_...
GITHUB_REPO=glen200392/ai-research-agent-team
GITHUB_BRANCH=main
GITHUB_COMMITTER_NAME=AI Research Federation Bot
GITHUB_COMMITTER_EMAIL=bot@nebula.gg

# AWS S3 (if OUTPUT_BACKEND=s3)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=ap-northeast-1
S3_BUCKET=

# Google Cloud Storage (if OUTPUT_BACKEND=gcs)
GCS_BUCKET=
GCS_PROJECT_ID=

# -----------------------------------------------------------------------------
# [RUNTIME] Date & Time Variables
# Automatically injected at pipeline start — do NOT hardcode in recipes
# -----------------------------------------------------------------------------
REPORT_DATE=                                 # Auto-set: YYYY-MM-DD (e.g. 2026-02-21)
REPORT_WEEK=                                 # Auto-set: YYYY-WXX  (e.g. 2026-W08)
REPORT_MONTH=                                # Auto-set: YYYY-MM   (e.g. 2026-02)
REPORT_YEAR=                                 # Auto-set: YYYY      (e.g. 2026)
TIMEZONE=Asia/Taipei                         # Pipeline execution timezone

# -----------------------------------------------------------------------------
# [PIPELINE] Feature Flags
# -----------------------------------------------------------------------------
ENABLE_EVOLUTION_CHRONICLE=true              # Team B: update evolution-graph.json
ENABLE_PEDAGOGY_GENERATION=true              # Team C: generate 3-level lessons
ENABLE_HITL_APPROVAL=true                    # Meta Harness: require human approval
ENABLE_GITHUB_UPLOAD=true                    # Auto-upload outputs to GitHub
ENABLE_JSON_SCHEMA_VALIDATION=true           # Validate JSON outputs before use
ENABLE_FALLBACK_ON_SEARCH_FAIL=true         # Use LLM knowledge if search fails

# -----------------------------------------------------------------------------
# [PIPELINE] Quality Thresholds (McKinsey Standard)
# -----------------------------------------------------------------------------
MIN_QUALITY_SCORE=75                         # Minimum acceptable team score (0-100)
UPGRADE_TRIGGER_THRESHOLD=70                 # Score below this triggers upgrade_needed
COVERAGE_MINIMUM_SOURCES=3                   # Minimum search result sources per step
SYNTHESIS_MIN_WORD_COUNT=800                 # Minimum words for weekly synthesis
LESSON_L1_MIN_WORDS=400                      # Level 1 minimum word count
LESSON_L2_MIN_WORDS=1000                     # Level 2 minimum word count
LESSON_L3_MIN_WORDS=2000                     # Level 3 minimum word count
QUIZ_QUESTION_COUNT=10                       # Number of quiz questions

# -----------------------------------------------------------------------------
# [PIPELINE] Retry & Resilience
# -----------------------------------------------------------------------------
MAX_RETRY_ATTEMPTS=3                         # Max retries per step on failure
RETRY_BACKOFF_SECONDS=30                     # Seconds between retries
SEARCH_TIMEOUT_SECONDS=30                    # Web search timeout
LLM_TIMEOUT_SECONDS=120                      # LLM agent response timeout
CIRCUIT_BREAKER_THRESHOLD=3                  # Consecutive failures before circuit open

# -----------------------------------------------------------------------------
# [OBSERVABILITY] Logging & Monitoring
# -----------------------------------------------------------------------------
LOG_LEVEL=INFO                               # DEBUG | INFO | WARN | ERROR
LOG_FORMAT=json                              # json | text
LOG_OUTPUT=stdout                            # stdout | file | both
LOG_FILE_PATH=logs/federation.log

ENABLE_METRICS=true                          # Emit pipeline execution metrics
METRICS_BACKEND=prometheus                   # prometheus | datadog | cloudwatch
PROMETHEUS_PUSH_GATEWAY=                     # Optional Prometheus pushgateway URL

# -----------------------------------------------------------------------------
# [FRAMEWORK] Execution Environment
# -----------------------------------------------------------------------------
EXECUTION_FRAMEWORK=nebula                   # nebula | langgraph | autogen | crewai | n8n
AGENT_CONCURRENCY_LIMIT=5                    # Max concurrent agent executions
PIPELINE_TIMEOUT_MINUTES=60                  # Hard timeout for entire pipeline

# LangGraph-specific (if EXECUTION_FRAMEWORK=langgraph)
LANGGRAPH_CHECKPOINT_BACKEND=memory         # memory | sqlite | postgres
LANGGRAPH_POSTGRES_URL=                      # Only if checkpoint_backend=postgres

# AutoGen-specific (if EXECUTION_FRAMEWORK=autogen)
AUTOGEN_CACHE_SEED=42                        # Reproducibility seed

# =============================================================================
# END OF CONFIGURATION
# =============================================================================
