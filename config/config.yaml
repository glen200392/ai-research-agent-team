# ============================================================
# AI Research Agent Team — Main Configuration
# All values here can be overridden by environment variables
# ============================================================

# ------------------------------------------------------------
# Report Settings
# ------------------------------------------------------------
report:
  language: "zh-TW"              # zh-TW | en | ja
  target_month: "auto"           # "auto" = previous month, or "2026-02"
  output_formats:
    - long_form                  # 2000-3000 word full report
    - linkedin_post              # 600-900 words
    - email_digest               # 400-500 words
  chapter_structure:
    - "模型發佈"
    - "開源生態"
    - "技術突破"
    - "硬體與基礎設施"
    - "AI Agent 架構演進"
    - "競爭格局分析"

# ------------------------------------------------------------
# Intelligence Collection
# ------------------------------------------------------------
intel:
  streams:
    A_academic:
      enabled: true
      sources:
        - arxiv.org
        - semanticscholar.org
        - paperswithcode.com
      queries_per_run: 5
      min_composite_score: 18     # Out of 30
    B_labs:
      enabled: true
      sources:
        - openai.com/blog
        - anthropic.com/news
        - deepmind.google/discover/blog
        - meta.ai
        - mistral.ai/news
      queries_per_run: 5
      min_composite_score: 20
    C_opensource:
      enabled: true
      sources:
        - huggingface.co/blog
        - github.com/trending
        - paperswithcode.com
      queries_per_run: 4
      min_composite_score: 18
    D_hardware:
      enabled: true
      sources:
        - nvidia.com/newsroom
        - amd.com/news
        - semianalysis.com
      queries_per_run: 4
      min_composite_score: 17
    E_breakthroughs:
      enabled: true
      sources:
        - nature.com
        - science.org
        - news.mit.edu
      queries_per_run: 4
      min_composite_score: 18
    F_industry:
      enabled: true
      sources:
        - crunchbase.com
        - reuters.com
        - techcrunch.com
        - venturebeat.com
      queries_per_run: 4
      min_composite_score: 17

# ------------------------------------------------------------
# Quality Gate
# ------------------------------------------------------------
quality:
  min_score: 85                  # 0-100; reports below this score are revised
  max_revisions: 2               # After this many cycles, force-approve with warning
  spot_check_searches: 5         # Max web searches QA can run for fact-checking
  layer_weights:
    factual_accuracy: 40
    structural_completeness: 30
    editorial_quality: 30

# ------------------------------------------------------------
# Delivery
# ------------------------------------------------------------
delivery:
  email:
    enabled: false               # Set to true only after configuring REPORT_EMAIL_RECIPIENT
    recipient: ""                # Set via REPORT_EMAIL_RECIPIENT env var
    subject_variant: "A"        # A or B
    attach_full_report: false
    format: "html"              # html | plain

  file_storage:
    enabled: true
    output_dir: "./reports/"
    filename_pattern: "AI_Tech_Report_{YYYY}_{MM}.md"
    save_all_formats: true      # Save linkedin + email variants too

  webhook:
    enabled: false
    url: ""                     # Set via DELIVERY_WEBHOOK_URL env var

  slack:
    enabled: false
    channel: "#ai-research"

  notion:
    enabled: false
    database_id: ""             # Set via NOTION_DATABASE_ID env var

# ------------------------------------------------------------
# LLM Settings
# ------------------------------------------------------------
llm:
  # Provider: anthropic | openai | google | ollama | lmstudio | openai_compatible | auto
  # "auto" = detect from env vars in priority order (Anthropic → OpenAI → Google → Ollama → LM Studio)
  provider: "auto"
  temperature: 0.3              # Lower = more factual, less creative
  max_tokens: 8192
  timeout_seconds: 120

  # Per-agent model overrides (optional)
  # Set provider + model per agent, or leave null to inherit global settings.
  # Example: use cheap local model for search, best cloud model for writing.
  agent_models:
    intel_collector: null       # e.g. {provider: ollama, model: llama3.2}
    tech_analyst: null
    market_analyst: null
    content_synthesizer: null   # e.g. {provider: anthropic, model: claude-opus-4-6}
    quality_gate: null
    delivery_agent: null
    archaeology_agent: null     # Team B
    evolution_linker: null      # Team B
    lesson_designer: null       # Team C
    quiz_generator: null        # Team C

# ------------------------------------------------------------
# Local Model Settings
# (all values overridable via environment variables)
# ------------------------------------------------------------
local_models:
  ollama:
    base_url: "http://localhost:11434"   # Override: OLLAMA_BASE_URL
    default_model: "llama3.2"           # Override: OLLAMA_MODEL
  lmstudio:
    base_url: "http://localhost:1234"    # Override: LMSTUDIO_BASE_URL
    default_model: "local-model"        # Override: LMSTUDIO_MODEL
  openai_compatible:
    base_url: ""                        # Set via: CUSTOM_API_BASE_URL
    model: ""                           # Set via: CUSTOM_MODEL
    api_key: ""                         # Set via: CUSTOM_API_KEY

# ------------------------------------------------------------
# Team B — Evolution Chronicle Settings
# ------------------------------------------------------------
team_b:
  evolution_graph_path: "docs/evolution-chronicle/evolution-graph.json"
  chronicle_dir: "docs/evolution-chronicle"
  auto_detect_latest_report: true     # Automatically find newest Team A report

# ------------------------------------------------------------
# Team C — Pedagogy Federation Settings
# ------------------------------------------------------------
team_c:
  lesson_output_dir: "pedagogy/weekly-lessons"
  levels: [1, 2, 3]                   # Which levels to generate (subset allowed)
  quiz_questions: 10                  # Total quiz questions (3/4/3 split by level)

# ------------------------------------------------------------
# Pipeline Behavior
# ------------------------------------------------------------
pipeline:
  parallel_analysis: true       # Run Tech + Market analysts in parallel
  timeout_per_agent_minutes: 15
  retry_on_stream_failure: true
  stream_retry_threshold: 2     # Retry if stream yields fewer than N sources
  save_intermediate_results: true
  intermediate_dir: "./tmp/pipeline/"
